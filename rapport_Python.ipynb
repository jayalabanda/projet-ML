{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Projet de Machine Learning</center></h1>\n",
    "\n",
    "Notebook <b>Python</b> avec les codes utilisés pour le rapport final.<br>\n",
    "Auteurs : Juan AYALA, Jeong Hwan KO, Alice LALOUE, Aldo MELLADO AGUILAR.<br>\n",
    "4A MA - Groupes A et B<br>\n",
    "2020 - 2021\n",
    "\n",
    "\n",
    "<p>Lien <a href=\"https://github.com/jayalabanda/projet-ML\">Github</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from functions import *\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtenir les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data = pd.read_csv(\"data/spotify-extr.txt\", sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description de l'ensemble du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On n'a pas de valeurs manquantes donc on n'a pas besoin de les retravailler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables explicatives sont :\n",
    "* `valence` : la positivité de la chanson, vaut 1 si la chanson est très joyeuse, 0 sinon ;\n",
    "* `year` : année de sortie ;\n",
    "* `acousticness` : mesure \"l'acousticité\" de la chanson ;\n",
    "* `danceability` : mesure la \"dançabilite\" d'une chanson ;\n",
    "* `duration` : durée d'une chanson en millisecondes ;\n",
    "* `energy` : l'énergie de la chanson, vaut 1 si la chanson est très énergétique, 0 sinon ;\n",
    "* `intrumentalness` : taux d'instrumentalisation, vaut 1 s'il n'y a aucune voix présente dans la chanson, 0 sinon ; \n",
    "* `key` : tonalité de la musique (ex : A=la), ne prend pas en compte la distinction majeur/mineur ;\n",
    "* `liveness` : taux de prestation en live, vaut 1 si la chanson ne comporte que de la musique (sans sons à intérêts non-musicaux), 0 sinon ;\n",
    "* `loudness` : intensité sonore de la chanson\n",
    "* `mode` : variable binaire qui indique si la chanson commence par une progression d'accords majeure (1) ou non (0)\n",
    "* `speechiness` : taux de vocaux dans la chanson, vaut 1 si la chanson comporte de la voix tout le long, 0 sinon ;\n",
    "* `tempo` :  tempo de la chanson en beats par minute (bpm)\n",
    "\n",
    "Notre objectif consiste à prédire la valeur de `pop.class` et de `popularity`, c'est-à-dire la popularité d'une chanson, soit comme un entier entre 0 et 100, soit comme une classe $A$, $B$, $C$ ou $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre jeu de données, les variables qualitatives sont :\n",
    "* `pop.class`,\n",
    "* `key`,\n",
    "* `mode`.\n",
    "\n",
    "Le reste des variables sont quantitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs de 'pop.class' :\", sorted(set(spotify_data[\"pop.class\"].values)), \"\\n\")\n",
    "print(\"Valeurs de 'key':\", sorted(set(spotify_data[\"key\"].values)), \"\\n\")\n",
    "print(\"Valeurs de 'mode' :\", set(spotify_data[\"mode\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme les variables qualitatives en catégories pour mieux traiter les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data[\"key\"] = pd.Categorical(spotify_data[\"key\"], ordered=False)\n",
    "spotify_data[\"mode\"] = pd.Categorical(spotify_data[\"mode\"], ordered=False)\n",
    "spotify_data[\"pop.class\"] = pd.Categorical(spotify_data[\"pop.class\"],\n",
    "                                           ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses uni et multidimensionnelles\n",
    "\n",
    "## Variables qualitatives\n",
    "\n",
    "On commence par analyser les variables qualitatives `pop.class`, `key` et `mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qual = spotify_data[[\"pop.class\", \"mode\", \"key\"]]\n",
    "data_qual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Classe de popularité</b> (variable à prédire)\n",
    "\n",
    "Cette variable a été créée en amont de l'obtention des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_class_count = data_qual[\"pop.class\"].value_counts(normalize=True)\n",
    "\n",
    "sns.barplot(x=pop_class_count.index, y=pop_class_count.values)\n",
    "# plt.title(\"Fréquence des classes de popularité\", fontsize=14)\n",
    "plt.ylabel(\"% d'occurences\")\n",
    "plt.xlabel(\"Classe\")\n",
    "save_fig(\"pop_class_frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'il y a une distribution plutôt uniforme des chansons par classe, sauf pour la classe `A`, qui comprend moins de 10% des chansons. Ceci risque de poser problème dans la suite en termes de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Clé</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "key_count = spotify_data['key'].value_counts(\n",
    "    normalize=True, sort=True, ascending=True) * 100\n",
    "y_ticks = spotify_data['key'].value_counts().index\n",
    "\n",
    "sns.barplot(x=key_count.values, y=y_ticks, data=key_count, orient='h')\n",
    "plt.xlabel(\"% d'occurences\")\n",
    "plt.ylabel('Clé')\n",
    "ax.set_xticks(ticks=range(0, 16, 1))\n",
    "ax.set_yticklabels(labels=y_ticks, fontsize=12)\n",
    "\n",
    "rects = ax.patches\n",
    "for rect in rects:\n",
    "    x_value = rect.get_width()\n",
    "    y_value = rect.get_y() + rect.get_height() / 2\n",
    "    label = f'{x_value:.1f}%'\n",
    "\n",
    "    plt.annotate(label, (x_value, y_value),\n",
    "                 xytext=(5, 0), textcoords=\"offset points\",\n",
    "                 va='center', ha='left')\n",
    "\n",
    "#plt.title(\"Distribution de 'key'\", fontsize=14)\n",
    "save_fig('keys_frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='key', y='popularity', data=spotify_data)\n",
    "#plt.title(\"Popularité selon la clé\", fontsize=14)\n",
    "plt.ylabel(\"Popularité moyenne\")\n",
    "plt.xlabel(\"Clé\")\n",
    "save_fig(\"popularity_by_key\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variances de la popularité dans chacune des valeurs de `key` est petite donc nous n'avons pas besoin de transformer ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='key', y='popularity', data=spotify_data)\n",
    "#plt.title(\"Popularité selon la clé\", fontsize=14)\n",
    "plt.ylabel(\"Popularité\")\n",
    "plt.xlabel(\"Clé\")\n",
    "save_fig(\"boxplot_of_popularity_by_key\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même façon, la distribution de la popularité reste plutôt uniforme par clé : les boîtes ont une taille similaire et la médiane est au même niveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Mode</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_count = spotify_data[\"mode\"].value_counts(normalize=True)\n",
    "\n",
    "sns.barplot(x=mode_count.index, y=mode_count.values)\n",
    "#plt.title(\"Fréquence des modes\", fontsize=14)\n",
    "plt.ylabel(\"% d'occurences\", fontsize=13)\n",
    "plt.xlabel(\"Mode\", fontsize=13)\n",
    "save_fig(\"mode_frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribution de la variable `mode` est inégale : il y a 30% et 70% des chansons avec `mode` = 0 et `mode` = 1 respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='mode', y='popularity', data=spotify_data)\n",
    "#plt.title(\"Fréquence des modes\", fontsize=14)\n",
    "plt.ylabel(\"Popularité moyenne\", fontsize=13)\n",
    "plt.xlabel(\"Mode\", fontsize=13)\n",
    "save_fig(\"popularity_by_mode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre, la popularité est similaire selon le mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='mode', y='popularity', data=spotify_data)\n",
    "#plt.title(\"Popularité selon la clé\", fontsize=14)\n",
    "plt.ylabel(\"Popularité\")\n",
    "plt.xlabel(\"Clé\")\n",
    "save_fig(\"boxplot_of_popularity_by_mode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regroupe toutes les variables qualitatives en un barplot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x='mode', y='popularity', hue='key', data=spotify_data)\n",
    "#plt.title(\"Popularité selon la clé et le mode\", fontsize=14)\n",
    "plt.ylabel(\"Popularité moyenne\")\n",
    "save_fig(\"popularity_by_key_and_mode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(x='mode', y='popularity', hue='key', data=spotify_data)\n",
    "#plt.title(\"Popularité selon la clé et le mode\", fontsize=14)\n",
    "plt.ylabel(\"Popularité\")\n",
    "save_fig(\"boxplot_popularity_by_key_and_mode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables quantitatives\n",
    "\n",
    "On commence par visualiser la corrélation entre les variables quantitatives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quant = spotify_data[spotify_data.columns.difference(\n",
    "    ['key', 'mode', 'pop.class'], sort=False)]\n",
    "data_quant.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data_quant.corr()\n",
    "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "hm = sns.heatmap(corr_matrix, cmap=cmap, center=0, linewidths=1, linecolor='gray')\n",
    "hm.set_yticklabels(hm.get_yticklabels(), fontsize=11)\n",
    "hm.set_xticklabels(hm.get_xticklabels(), rotation=45, fontsize=11, ha='right')\n",
    "#plt.title(\"Matrice de corrélation\")\n",
    "save_fig(\"correlation_square_matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce graphique nous montre qu'il y a certaines variables qui ont une forte corrélation. Par exemple, il y a une forte corrélation négative entre les variables `energy` et `acousticness`. Cela a du sens vu que les chansons acoustiques sont plus tranquilles (moins énergiques) que celles qui ne sont pas acoustiques. De même, `energy` et `loudness` sont positivement corrélées, ce qui est attendu vu que les chansons bruyantes ont souvent plus d'énergie.\n",
    "<br>\n",
    "On voit aussi que plus une chanson est acoustique, moins elle est populaire, vu que les variables `acousticness` et `popularity` ont une forte corrélation négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = np.abs(corr_matrix['popularity']).sort_values(ascending=False)\n",
    "print(\"Les variables les plus corrélées avec la variable 'popularity' sont : \")\n",
    "for i, row in enumerate(series):\n",
    "    if 0.2 <= row < 1:\n",
    "        print(f'{series.index[i]:17} --> {row: .2f} (abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici leurs distributions avec boxplot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-poster')\n",
    "\n",
    "fig = plt.figure(figsize=(22, 28))\n",
    "outer = fig.add_gridspec(6, 2, wspace=0.1, hspace=0.5, left=0.03,\n",
    "                         right=0.98, bottom=0.03, top=0.98)\n",
    "\n",
    "a = 0\n",
    "for i in range(6):\n",
    "    for j in range(2):\n",
    "        feature = data_quant.columns[a]\n",
    "        inner = outer[i, j].subgridspec(2, 1, wspace=0.2, hspace=0,\n",
    "                                        height_ratios=[0.15, 0.85])\n",
    "        axs = inner.subplots(sharex=True)\n",
    "\n",
    "        sns.boxplot(data=data_quant, x=feature, orient='h', ax=axs[0])\n",
    "        sns.histplot(data=data_quant, x=feature, bins=50 if a != 1 else 100,\n",
    "                     ax=axs[1], kde=True)\n",
    "\n",
    "        axs[0].spines['top'].set_color('black')\n",
    "        axs[0].spines['right'].set_color('black')\n",
    "        axs[0].spines['left'].set_color('black')\n",
    "\n",
    "        axs[1].set_title(\"Distribution de '\" + feature + \"'\", y=1.2, fontsize=14)\n",
    "        axs[1].spines['bottom'].set_color('black')\n",
    "        axs[1].spines['right'].set_color('black')\n",
    "        axs[1].spines['left'].set_color('black')\n",
    "\n",
    "        a += 1\n",
    "\n",
    "    #fig.suptitle('Distribution des variables quantitatives', y=1.01, fontsize=20)\n",
    "save_fig('hist_boxplot_of_data', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une étude plus approfondie de chaque variable quantitative :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Acousticness</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_data = spotify_data.groupby('acousticness')['popularity'].mean().to_frame().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = sns.scatterplot(x=ax_data['acousticness'], y=ax_data['popularity'], color='blue')\n",
    "sc.tick_params(labelsize=12)\n",
    "#plt.title(\"Acousticité\")\n",
    "plt.ylabel('Popularité moyenne')\n",
    "save_fig('mean_popularity_by_acousticness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Danceability</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_data = spotify_data.groupby('danceability')['popularity'].mean().to_frame().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = sns.scatterplot(x='danceability', y='popularity', data=ax_data, color='blue')\n",
    "sc.tick_params(labelsize=12)\n",
    "#plt.title('Dançabilité')\n",
    "plt.ylabel('Popularité moyenne')\n",
    "save_fig('mean_popularity_by_danceability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Duration</b>\n",
    "\n",
    "On convertit la durée des chansons en minutes pour en tirer plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data['duration'] = spotify_data['duration'] / 60000\n",
    "spotify_data['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "hp = sns.histplot(spotify_data['duration'], bins=60, kde=False)\n",
    "hp.tick_params(labelsize=12)\n",
    "plt.xlabel('duration (mins)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que la chanson la plus longue dans le jeu de données dure 45 minutes, donc on choisit de séparer les chansons longues de chansons courtes au seuil de 7 minutes pour mieux voir les durées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_songs = spotify_data.loc[spotify_data['duration'] > 7]\n",
    "short_songs = spotify_data.loc[spotify_data['duration'] <= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "hp = sns.histplot(short_songs['duration'], kde=False, bins=80)\n",
    "hp.tick_params(labelsize=12)\n",
    "#plt.title(f'Chansons courtes (<=7 mins) : {short_songs.shape[0]} chansons')\n",
    "plt.xlabel('duration (mins)')\n",
    "save_fig('hist_of_short_songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "hp = sns.histplot(long_songs['duration'], kde=False, bins=50)\n",
    "hp.tick_params(labelsize=12)\n",
    "#plt.title(f'Chansons longues (>7 mins) : {long_songs.shape[0]} chansons')\n",
    "plt.xlabel('duration (mins)')\n",
    "save_fig('hist_of_long_songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1 = sns.histplot(short_songs['duration'], kde=False, bins=60, ax=ax1)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticks(range(0, 8, 1))\n",
    "ax1.tick_params(labelsize=12)\n",
    "\n",
    "ax2 = sns.histplot(long_songs['duration'], kde=False, bins=50, ax=ax2)\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_xticks(range(10, 46, 5))\n",
    "ax2.tick_params(labelsize=12)\n",
    "\n",
    "plt.text(0.5, -20, 'duration (mins)', ha='center', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "ax1_data = short_songs.groupby('duration')['popularity'].mean().to_frame().reset_index()\n",
    "ax1 = sns.scatterplot(x='duration', y='popularity', data=ax1_data, color='blue', ax=ax1)\n",
    "ax1.set_xlim(-0.2, 7.2)\n",
    "ax1.set_ylabel('Popularité moyenne')\n",
    "ax1.set_xlabel('')\n",
    "# ax1.set_title('Chansons courtes')\n",
    "\n",
    "ax2_data = long_songs.groupby('duration')['popularity'].mean().to_frame().reset_index()\n",
    "ax2 = sns.scatterplot(x=ax2_data['duration'], y=ax2_data['popularity'], color='orange', ax=ax2)\n",
    "ax2.set_xticks(range(7, 46, 4))\n",
    "ax2.set_xlabel('')\n",
    "# ax2.set_title('Chansons longues')\n",
    "\n",
    "plt.suptitle('duration (mins)', x=0.53, y=-0.01, ha='center', fontsize=16)\n",
    "save_fig('popularity_by_long_short_songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Energy</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_data = spotify_data.groupby('energy')['popularity'].mean().to_frame().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = sns.scatterplot(x='energy', y='popularity', data=ax_data, color='blue')\n",
    "sc.tick_params(labelsize=12)\n",
    "#plt.title('Énergie')\n",
    "plt.ylabel('Popularité moyenne')\n",
    "save_fig('mean_popularity_by_energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instrumentalness</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data['instrumentalness'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.loc[spotify_data['instrumentalness'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `instrumentalness` a une répartition très inégale : presque 30% des chansons ont une valeur d'instrumentalité de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "vp = sns.violinplot(x=\"instrumentalness\", data=spotify_data)\n",
    "vp.tick_params(labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_data = spotify_data.groupby('instrumentalness')['popularity'].mean().to_frame().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = sns.scatterplot(x='instrumentalness', y='popularity', data=ax_data, color='blue')\n",
    "sc.tick_params(labelsize=12)\n",
    "#plt.title('Instrumentalité')\n",
    "plt.ylabel('Popularité moyenne')\n",
    "save_fig('mean_popularity_by_instrumentalness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Liveness</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_data = spotify_data.groupby('liveness')['popularity'].mean().to_frame().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = sns.scatterplot(x='liveness', y='popularity', data=ax_data, color='blue')\n",
    "sc.tick_params(labelsize=12)\n",
    "#plt.title('liveness')\n",
    "plt.ylabel('Popularité moyenne')\n",
    "save_fig('mean_popularity_by_liveness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Popularity</b> (variable à prédire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.loc[spotify_data[\"popularity\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data[\"popularity\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'il y a un nombre important de chansons ayant 0 comme popularité. En effet ces chansons sont proches de l'extraction de la base des données et donc leur popularité n'avait pas encore été déterminée.\n",
    "\n",
    "De plus, la moitié des chansons a une popularité entre 11 et 48. Ceci posera aussi des problèmes lors de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 11))\n",
    "ax1 = sns.histplot(spotify_data['popularity'], ax=ax1, bins=50)\n",
    "ax2 = sns.histplot(spotify_data.loc[spotify_data['popularity'] > 0, 'popularity'],\n",
    "                   ax=ax2, bins=50)\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "plt.suptitle('Haut : Toutes les données\\nBas : Popularité > 0', fontsize=16)\n",
    "save_fig('hist_of_popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax = spotify_data.groupby('year')['popularity'].mean().plot()\n",
    "#ax.set_title('Popularité moyenne au cours des années')\n",
    "ax.set_ylabel('Popularité moyenne', fontsize=13)\n",
    "ax.set_xlabel('Année')\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_xticks(range(1920, 2021, 5))\n",
    "save_fig('mean_popularity_by_year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tempo</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='tempo', y='popularity', data=spotify_data, height=8)\n",
    "save_fig('jointplot_of_tempo_popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.loc[spotify_data['tempo'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'il y a 13 chansons pour lesquelles `tempo` vaut 0 ce qui n'est pas possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tempo = spotify_data.loc[spotify_data['tempo'] > 0]['tempo']\n",
    "corrected_tempo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax = sns.histplot(spotify_data['tempo'], bins=200, kde=False)\n",
    "ax.set_ylabel('Fréquences', fontsize=12)\n",
    "\n",
    "ax.text(s='13\\nOutliers', x=5, y=40, fontdict={'size': 12, 'c': 'darkred'})\n",
    "ax.text(s='Valeurs sans 0', x=125, y=160,\n",
    "        fontdict={'size': 12, 'c': 'darkred'})\n",
    "ax.text(s='Médiane\\ncorrigée\\n114.55', x=116, y=40,\n",
    "        fontdict={'size': 10, 'c': 'darkgreen', 'weight': 'bold'})\n",
    "\n",
    "ax.axvline(x=114.55, ymin=0, ymax=0.7, color='green',\n",
    "           linestyle='dashed', linewidth=2)\n",
    "ax.axvline(x=35.37, ymin=0, ymax=1, color='orange',\n",
    "           linestyle='dashed', linewidth=3)\n",
    "ax.axvline(x=214.42, ymin=0, ymax=1, color='orange',\n",
    "           linestyle='dashed', linewidth=3)\n",
    "\n",
    "ax.annotate(\"\", xy=(35.37, 150), xytext=(214.42, 150),\n",
    "            arrowprops=dict(arrowstyle=\"<->\",\n",
    "                            color='r',\n",
    "                            linestyle='dashed',\n",
    "                            linewidth=2))\n",
    "ax.annotate(\"\", xy=(0, 30), xytext=(0, 50),\n",
    "            arrowprops=dict(arrowstyle=\"->\",\n",
    "                            color='r',\n",
    "                            linestyle='dashed',\n",
    "                            linewidth=3))\n",
    "\n",
    "save_fig('distribution_of_tempo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On replace les valeurs où `tempo` = 0 par la médiane dans la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = spotify_data.loc[spotify_data[\"tempo\"] > 0, \"tempo\"].median()\n",
    "print(median)\n",
    "\n",
    "spotify_data.replace(to_replace={\"tempo\": 0}, value=median, inplace=True)\n",
    "spotify_data[\"tempo\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Year</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='year', y='popularity', data=spotify_data, height=10)\n",
    "#plt.suptitle(\"Joint plot de la popularité selon l'année de sortie\", y=1.02)\n",
    "save_fig(\"jointplot_of_popularity_by_year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prétraitement de variables :</b>\n",
    "\n",
    "On normalise la variable `danceability` vu sa ressemblance à une loi gaussienne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data[\"dance_norm\"] = (spotify_data[\"danceability\"] - spotify_data[\"danceability\"].mean())\\\n",
    "    / spotify_data[\"danceability\"].std()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "hp = sns.histplot(spotify_data[\"dance_norm\"], bins=50, kde=True)\n",
    "hp.tick_params(labelsize=12)\n",
    "#plt.title(\"Variable 'danceability' normalisée\")\n",
    "save_fig(\"scaled_danceability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on la supprime :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del spotify_data[\"danceability\"]\n",
    "spotify_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces deux cellules prennent assez de temps à s'exécuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in ['key', 'mode']:\n",
    "#     sns.pairplot(spotify_data, hue=i)\n",
    "#     t = 'pairplot_of_data_by_' + i\n",
    "#     save_fig(t)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(spotify_data)\n",
    "# plt.suptitle(\"Pair plot des données\", fontsize=20, y=1.02)\n",
    "# save_fig(\"pairplot_of_dataset\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "attributs = [\n",
    "    feature for feature in spotify_data.keys()\n",
    "    if feature not in data_qual.keys()\n",
    "    and feature != 'popularity'\n",
    "]\n",
    "print(attributs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = spotify_data[attributs]\n",
    "X_scaled = scale(X_new)\n",
    "pca = PCA(random_state=42)\n",
    "spotify_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(pca.explained_variance_.size)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "var_ratio = pca.explained_variance_ratio_\n",
    "labels = ['Dim ' + str(i + 1) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10), sharex=True)\n",
    "\n",
    "ax[0].bar(x, var_ratio)\n",
    "ax[0].plot(var_ratio, color='black')\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(labels, fontsize=12)\n",
    "ax[0].set_ylabel(\"Pourcentage de la variance expliquée\", fontsize=16)\n",
    "#ax[0].set_title(\"Part de la variance expliquée\", fontsize=15)\n",
    "\n",
    "for p in ax[0].patches:\n",
    "    text = str(np.round(p.get_height(), 3) * 100)[:4] + '%'\n",
    "    ax[0].annotate(text=text,\n",
    "                   xy=(p.get_x() + p.get_width() / 2., p.get_height() + 0.01),\n",
    "                   fontsize='large', ha='center', va='center')\n",
    "\n",
    "ax[1].bar(x, cumsum, width=.7)\n",
    "ax[1].plot(x, cumsum)\n",
    "ax[1].set_ylabel(\"Variance partagée\", fontsize=16)\n",
    "ax[1].set_xticklabels(labels, fontsize=12)\n",
    "#ax[1].set_title(\"Somme cumulée de la part de la variance\", fontsize=15)\n",
    "\n",
    "for p in ax[1].patches:\n",
    "    text = str(np.round(p.get_height(), 3) * 100)[:4] + '%'\n",
    "    ax[1].annotate(text=text,\n",
    "                   xy=(p.get_x() + p.get_width() / 2., p.get_height() + 0.01),\n",
    "                   fontsize='large', ha='center', va='center')\n",
    "\n",
    "fig.text(0.5, -0.03, \"Composantes Principales\", ha='center', fontsize=20)\n",
    "#plt.suptitle(\"Analyse de la variance des composantes principales\", fontsize=22)\n",
    "save_fig(\"explained_var_ratio_and_cumulative\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.boxplot(spotify_pca)\n",
    "plt.axhline(color='grey', linewidth=1, linestyle='--')\n",
    "#plt.title(\"Boxplot des variables de l'ACP\")\n",
    "save_fig(\"boxplot_of_variances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sélection de variables :\n",
    "On sélectionne les 6 premières composantes principales.\n",
    "Variance expliquée par les valeurs propres : 80% de variance expliquée à partir de 6 CP\n",
    "On observe un coude sur le graphe des variances expliquées à partir de la 6e CP.\n",
    "Boxplots : étendue des boxplots relativement stable à partir de la 5 ou 6e CP, la médiane des boxplots devient relativement identique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=spotify_pca[:, 0], y=spotify_pca[:, 1],\n",
    "                hue='pop.class', data=spotify_data, alpha=.7)\n",
    "plt.legend(title='Classe de popularité',\n",
    "           title_fontsize=13, fontsize=12)\n",
    "plt.axvline(color=\"grey\", linewidth=1)\n",
    "plt.axhline(color=\"grey\", linewidth=1)\n",
    "plt.xlabel('Dim 1')\n",
    "plt.ylabel('Dim 2')\n",
    "#plt.title(\"Nuage de points des individus de l'ACP\")\n",
    "save_fig(\"scatterplot_of_individuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Nuage de points des individus:\n",
    "On observe 2 groupes distincts : 1 grand et un plus petit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_circle(X_new, pca, 1, 2)\n",
    "save_fig(\"pca_components_1_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_circle(X_new, pca, 1, 3)\n",
    "save_fig(\"pca_components_1_3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cercle des correlations  (dim 1 et dim 2):\n",
    "\n",
    "Variables représentées par les flèches.\n",
    "\n",
    "Speechiness : entièrement expliquée par la dimension 2.\n",
    "Log_duration et speechiness sont très proches de l'axe des ordonnées : variables expliquées en majorité par la dimension 2.\n",
    "Instrumentalness, accousticness, loudness: essetiellement expliquées par la dimension 1.\n",
    "\n",
    "Accousticness et loudness : flèches sur le même axe. Variables inversement corrélées. En accord avec le graphe des corrélations.\n",
    "\n",
    "Axe 2 : \"divise\" les flèches en 2 ?\n",
    "A droite du graphe : dans les valeurs positives, on retrouve les chansons plus calmes / accoustiques / instrumentales\n",
    "A gauche du graphe : dans les valeurs négatives , on retrouve les chansons plus \"loud\", dançantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mode = {1: 'Mode 0', 2: 'Mode 1'}\n",
    "\n",
    "def plot_pca(l_pca, fig, ax, nbc, nbc2, pca):\n",
    "    cmaps = plt.get_cmap(\"Set2\")\n",
    "    for i in range(2):\n",
    "        xs = l_pca[spotify_data[\"mode\"] == i, nbc - 1]\n",
    "        ys = l_pca[spotify_data[\"mode\"] == i, nbc2 - 1]\n",
    "        label = label_mode[i + 1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(x=xs, y=ys, color=color, alpha=.5, s=1, label=label)\n",
    "        ax.set_xlabel(\"PC %d: %.2f%%\" %\n",
    "                      (nbc, pca.explained_variance_ratio_[nbc - 1] * 100), fontsize=10)\n",
    "        ax.set_ylabel(\"PC %d: %.2f%%\" %\n",
    "                      (nbc2, pca.explained_variance_ratio_[nbc2 - 1] * 100), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "for nbc, nbc2, count in [(1, 2, 1), (1, 3, 2), (1, 4, 3),\n",
    "                         (2, 3, 5), (2, 4, 6), (3, 4, 9)]:\n",
    "    ax = fig.add_subplot(3, 3, count)\n",
    "    plot_pca(spotify_pca, fig, ax, nbc, nbc2, pca)\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "plt.legend(loc='best', bbox_to_anchor=(1.8, 2), markerscale=8, fontsize='large')\n",
    "#plt.suptitle('Principal Components from 1 to 4', fontsize=14)\n",
    "save_fig('scatter_of_1_4_pc', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_circle_bis(data, pca, comp1, comp2, fig, ax):\n",
    "    '''Plots correlation circle from results of PCA'''\n",
    "    coord1 = pca.components_[comp1 - 1] * np.sqrt(pca.explained_variance_[comp1 - 1])\n",
    "    coord2 = pca.components_[comp2 - 1] * np.sqrt(pca.explained_variance_[comp2 - 1])\n",
    "\n",
    "    cmap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "    for i, j, nom in zip(coord1, coord2, data.columns):\n",
    "        plt.text(i, j, nom, rotation=45)\n",
    "        rainbowarrow(ax, (0, 0), (i, j), cmap=cmap, lw=2)\n",
    "    plt.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "\n",
    "    # cercle\n",
    "    c = plt.Circle((0, 0), radius=1, color='gray', fill=False)\n",
    "    ax.add_patch(c)\n",
    "    \n",
    "    xlab = 'Dim ' + str(comp1) + ': ' + str(pca.explained_variance_ratio_[comp1 - 1] * 100)[:4] + '%'\n",
    "    ylab = 'Dim ' + str(comp2) + ': ' + str(pca.explained_variance_ratio_[comp2 - 1] * 100)[:4] + '%'\n",
    "    ax.set_xlabel(xlab, fontsize=12)\n",
    "    ax.set_ylabel(ylab, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "for nbc, nbc2, count in [(1, 2, 1), (1, 3, 2), (1, 4, 3), (2, 3, 5), (2, 4, 6), (3, 4, 9)]:\n",
    "    ax = fig.add_subplot(3, 3, count)\n",
    "    plot_corr_circle_bis(X_new, pca, nbc, nbc2, fig, ax)\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "\n",
    "#plt.suptitle('Principal Components from 1 to 4', fontsize=14)\n",
    "#save_fig('scatter_of_1_4_corr_circle', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "spotify_pop_class = spotify_data[[\"pop.class\"]]\n",
    "spotify_key = spotify_data[[\"key\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "spotify_pop_class_encoded = label_encoder.fit_transform(spotify_pop_class.values.ravel())\n",
    "print(spotify_pop_class_encoded[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(dtype=np.int32)\n",
    "\n",
    "spotify_key_encoded = ordinal_encoder.fit_transform(spotify_key)\n",
    "spotify_key_encoded = np.squeeze(spotify_key_encoded)\n",
    "print(spotify_key_encoded[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data[\"key\"] = spotify_key_encoded\n",
    "spotify_data[\"pop.class\"] = spotify_pop_class_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    feature for feature in spotify_data.keys()\n",
    "    if feature not in ['popularity', 'pop.class']\n",
    "]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spotify_data[features]\n",
    "\n",
    "y_class = spotify_data[[\"pop.class\"]]\n",
    "y_reg = spotify_data[[\"popularity\"]]\n",
    "y_class = y_class.values.ravel()\n",
    "y_reg = y_reg.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_reg[:15])\n",
    "print(y_class[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_model(n_inputs, n_outputs, problem=None):\n",
    "    '''Fonction pour créer un réseau de neuronnes'''\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(30,\n",
    "                                 input_dim=n_inputs,\n",
    "                                 activation='relu'))\n",
    "    \n",
    "#     model.add(keras.layers.Dense(150, activation='relu'))\n",
    "#     model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    \n",
    "    if problem == 'regression':\n",
    "        model.add(keras.layers.Dense(n_outputs,\n",
    "                                     activation='linear'))\n",
    "\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    elif problem == 'classification':\n",
    "        model.add(keras.layers.Dense(n_outputs,\n",
    "                                     activation='softmax'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "\n",
    "model_accuracy_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
    "    X, y_class, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled_mm = mm_scaler.fit_transform(X_train)\n",
    "X_test_scaled_mm = mm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sans pénalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg_Model = LogisticRegression(penalty='none', solver='saga',\n",
    "                                   multi_class='multinomial', max_iter=4000)\n",
    "Log_Reg_Model.fit(X_train_scaled, y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR_Predict = Log_Reg_Model.predict(X_test_scaled)\n",
    "LogR_Accuracy = accuracy_score(y_test_class, LogR_Predict)\n",
    "model_accuracy_score.append(LogR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(LogR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, LogR_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Logistic Regression\")\n",
    "save_fig(\"confusion_matrix_of_Log_Reg_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, LogR_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 1., 5., 10., 15.],\n",
    "}]\n",
    "\n",
    "Lasso_Model = GridSearchCV(\n",
    "    LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial',\n",
    "                       max_iter=4000, random_state=100),\n",
    "    param, cv=10)\n",
    "\n",
    "Lasso_Model.fit(X_train_scaled, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Lasso_Model.best_score_, Lasso_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_Predict = Lasso_Model.predict(X_test_scaled)\n",
    "Lasso_Accuracy = accuracy_score(y_test_class, Lasso_Predict)\n",
    "model_accuracy_score.append(Lasso_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(Lasso_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, Lasso_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Lasso Classification\")\n",
    "save_fig(\"confusion_matrix_of_Lasso_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, Lasso_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 1., 5., 10., 15.],\n",
    "    \"solver\": ['saga', 'lbfgs']\n",
    "}]\n",
    "\n",
    "Ridge_Model = GridSearchCV(\n",
    "    LogisticRegression(penalty='l2', multi_class='multinomial',\n",
    "                       max_iter=4000, random_state=100),\n",
    "    param, cv=10)\n",
    "\n",
    "Ridge_Model.fit(X_train_scaled, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Ridge_Model.best_score_, Ridge_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_Predict = Ridge_Model.predict(X_test_scaled)\n",
    "Ridge_Accuracy = accuracy_score(y_test_class, Ridge_Predict)\n",
    "model_accuracy_score.append(Ridge_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(Ridge_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, Ridge_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Ridge Classification\")\n",
    "save_fig(\"confusion_matrix_of_Ridge_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, Ridge_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 1., 5., 10., 15.],\n",
    "    \"l1_ratio\": [0.25, 0.5, 0.75]\n",
    "}]\n",
    "\n",
    "EN_Model = GridSearchCV(\n",
    "    LogisticRegression(penalty='elasticnet', solver='saga', multi_class='multinomial',\n",
    "                       max_iter=4000, random_state=42),\n",
    "    param, cv=10)\n",
    "\n",
    "EN_Model.fit(X_train_scaled, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (EN_Model.best_score_, EN_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_Predict = EN_Model.predict(X_test_scaled)\n",
    "EN_Accuracy = accuracy_score(y_test_class, EN_Predict)\n",
    "model_accuracy_score.append(EN_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(EN_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, EN_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Elastic Net Classification\")\n",
    "save_fig(\"confusion_matrix_of_ENet_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, EN_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"max_features\": [*range(2, 10), 'auto', 'log2']\n",
    "}]\n",
    "\n",
    "RFC_Model = GridSearchCV(\n",
    "    RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
    "    param, cv=5)\n",
    "\n",
    "RFC_Model.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (RFC_Model.best_score_, RFC_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_Predict = RFC_Model.predict(X_test)\n",
    "RFC_Accuracy = accuracy_score(y_test_class, RFC_Predict)\n",
    "model_accuracy_score.append(RFC_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(RFC_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, RFC_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Random Forest Classification\")\n",
    "save_fig(\"confusion_matrix_of_RF_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, RFC_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"min_samples_split\": range(2, 203, 10),\n",
    "    \"max_features\": [None, 'auto', 'log2']\n",
    "}]\n",
    "\n",
    "DT_Model = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                        param, cv=5)\n",
    "\n",
    "DT_Model.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleurs paramètres = %s\" %\n",
    "      (DT_Model.best_score_, DT_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_Predict = DT_Model.predict(X_test)\n",
    "DT_Accuracy = accuracy_score(y_test_class, DT_Predict)\n",
    "model_accuracy_score.append(DT_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(DT_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, DT_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Decision Trees Classification\")\n",
    "save_fig(\"confusion_matrix_of_DT_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, DT_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n",
    "\n",
    "#### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.01, 0.1, 0.5, 1., 2., 5., 10.]\n",
    "}]\n",
    "\n",
    "Lin_SVC_Model = GridSearchCV(\n",
    "    SVC(kernel='linear', decision_function_shape='ovo', max_iter=10_000, random_state=100),\n",
    "    param, cv=5\n",
    ")\n",
    "\n",
    "Lin_SVC_Model.fit(X_train_scaled_mm, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Lin_SVC_Model.best_score_, Lin_SVC_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin_SVC_Predict = Lin_SVC_Model.predict(X_test_scaled_mm)\n",
    "Lin_SVC_Accuracy = accuracy_score(y_test_class, Lin_SVC_Predict)\n",
    "model_accuracy_score.append(Lin_SVC_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(Lin_SVC_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, Lin_SVC_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Linear SVC Classification)\n",
    "save_fig(\"confusion_matrix_of_Lin_SVC_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, Lin_SVC_Predict, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [1, 10, 100, 1000],\n",
    "    \"degree\": [2, 3, 4]\n",
    "}]\n",
    "\n",
    "Poly_SVC_Model = GridSearchCV(\n",
    "    SVC(kernel='poly', gamma='auto', coef0=1., \n",
    "        decision_function_shape='ovo', random_state=2021),\n",
    "    param, cv=5\n",
    ")\n",
    "\n",
    "Poly_SVC_Model.fit(X_train_scaled_mm, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Poly_SVC_Model.best_score_, Poly_SVC_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Poly_SVC_Predict = Poly_SVC_Model.predict(X_test_scaled_mm)\n",
    "Poly_SVC_Accuracy = accuracy_score(y_test_class, Poly_SVC_Predict)\n",
    "model_accuracy_score.append(Poly_SVC_Accuracy)\n",
    "\n",
    "print(\"Précision :\" + str(Poly_SVC_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, Poly_SVC_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Polynomial SVC Classification\")\n",
    "save_fig(\"confusion_matrix_of_Poly_SVC_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, Poly_SVC_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [1, 10, 100, 1000],\n",
    "}]\n",
    "\n",
    "Rad_SVC_Model = GridSearchCV(\n",
    "    SVC(kernel='rbf', gamma='auto', decision_function_shape='ovo', random_state=2021),\n",
    "    param, cv=5\n",
    ")\n",
    "\n",
    "Rad_SVC_Model.fit(X_train_scaled_mm, y_train_class)\n",
    "\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Rad_SVC_Model.best_score_, Rad_SVC_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rad_SVC_Predict = Rad_SVC_Model.predict(X_test_scaled_mm)\n",
    "Rad_SVC_Accuracy = accuracy_score(y_test_class, Rad_SVC_Predict)\n",
    "model_accuracy_score.append(Rad_SVC_Accuracy)\n",
    "\n",
    "print(\"Précision :\" + str(Rad_SVC_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, Rad_SVC_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Radial SVC Classification\")\n",
    "save_fig(\"confusion_matrix_of_Rad_SVC_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, Rad_SVC_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_nn, X_valid, y_train_class_nn, y_valid = train_test_split(\n",
    "    X_train_scaled, y_train_class, train_size=0.8, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1], 4\n",
    "NN_Model = get_NN_model(n_inputs, n_outputs, 'classification')\n",
    "NN_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = NN_Model.fit(X_train_scaled_nn, y_train_class_nn, epochs=1000,\n",
    "                       batch_size=30, validation_data=(X_valid, y_valid),\n",
    "                       verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 2.5)\n",
    "    plt.plot(hist.epoch, hist.loss, label='Loss')\n",
    "    plt.plot(hist.epoch, hist.val_loss, label='Validation loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Predict = np.argmax(NN_Model.predict(X_test_scaled), axis=-1)\n",
    "NN_Accuracy = accuracy_score(y_test_class, NN_Predict)\n",
    "model_accuracy_score.append(NN_Accuracy)\n",
    "\n",
    "print(\"Précision :\" + str(NN_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, NN_Predict, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Neural Network Classification\")\n",
    "save_fig(\"confusion_matrix_of_NN_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, NN_Predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé des résultats en classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_models = [\n",
    "    'Logistic Regression', 'Lasso', 'Ridge', 'Elastic Net', 'Random Forest',\n",
    "    'Decision Trees', 'Linear SVC', 'Polynomial SVC', 'Radial SVC',\n",
    "    'Neural Network'\n",
    "]\n",
    "\n",
    "model_performance_accuracy = pd.DataFrame({\n",
    "    'Model': class_models,\n",
    "    'Accuracy Score': model_accuracy_score\n",
    "})\n",
    "\n",
    "model_performance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_accuracy.sort_values(by='Accuracy Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "reg_metrics = (mean_squared_error, r2_score, explained_variance_score)\n",
    "threshold_accuracy_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores = []\n",
    "r2_scores = []\n",
    "evs_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sans pénalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model = LinearRegression()\n",
    "LR_Model.fit(X_train, y_train_reg)\n",
    "LR_Predict = LR_Model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, LR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, LR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, LR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, LR_Predict)\n",
    "#plt.title(\"Results of Linear Regression\")\n",
    "save_fig(\"results_of_LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_LR = reg_to_class(LR_Predict)\n",
    "LR_Accuracy = accuracy_score(y_test_class, y_rtc_LR)\n",
    "threshold_accuracy_score.append(LR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(LR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_LR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Linear Regression\")\n",
    "save_fig(\"confusion_matrix_of_LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"alpha\": [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.]\n",
    "}]\n",
    "\n",
    "LassoR_Model = GridSearchCV(Lasso(), param, cv=10)\n",
    "LassoR_Model.fit(X_train, y_train_reg)\n",
    "LassoR_Predict = LassoR_Model.predict(X_test)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (LassoR_Model.best_score_, LassoR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, LassoR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, LassoR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, LassoR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, LassoR_Predict)\n",
    "#plt.title(\"Results of Lasso Regression\")\n",
    "save_fig(\"results_of_Lasso_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_LassoR = reg_to_class(LassoR_Predict)\n",
    "LassoR_Accuracy = accuracy_score(y_test_class, y_rtc_LassoR)\n",
    "threshold_accuracy_score.append(LassoR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(LassoR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_LassoR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Lasso Regression\")\n",
    "save_fig(\"confusion_matrix_of_Lasso_Reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"alpha\": [0.1, 0.5, 1., 1.5, 2., 3., 5., 10.]\n",
    "}]\n",
    "\n",
    "RidgeR_Model = GridSearchCV(Ridge(), param, cv=10)\n",
    "RidgeR_Model.fit(X_train, y_train_reg)\n",
    "RidgeR_Predict = RidgeR_Model.predict(X_test)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (RidgeR_Model.best_score_, RidgeR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, RidgeR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, RidgeR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, RidgeR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, RidgeR_Predict)\n",
    "#plt.title(\"Results of Ridge Regression\")\n",
    "save_fig(\"results_of_Ridge_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_RidgeR = reg_to_class(RidgeR_Predict)\n",
    "RidgeR_Accuracy = accuracy_score(y_test_class, y_rtc_RidgeR)\n",
    "threshold_accuracy_score.append(RidgeR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(RidgeR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_RidgeR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Ridge Regression\")\n",
    "save_fig(\"confusion_matrix_of_Ridge_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"alpha\": [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.]\n",
    "}]\n",
    "\n",
    "ENetR_Model = GridSearchCV(ElasticNet(), param, cv=10)\n",
    "ENetR_Model.fit(X_train, y_train_reg)\n",
    "ENetR_Predict = ENetR_Model.predict(X_test)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (ENetR_Model.best_score_, ENetR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, ENetR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, ENetR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, ENetR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, ENetR_Predict)\n",
    "#plt.title(\"Results of Elastic Net Regression\")\n",
    "save_fig(\"results_of_ENet_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_ENetR = reg_to_class(ENetR_Predict)\n",
    "ENetR_Accuracy = accuracy_score(y_test_class, y_rtc_ENetR)\n",
    "threshold_accuracy_score.append(ENetR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(ENetR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_ENetR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Elastic Net Regression\")\n",
    "save_fig(\"confusion_matrix_of_ENet_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation par validation croisée de la valeur de *max_features* et *min_samples_split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"max_features\": [*range(2, 10), 'auto', 'log2'],\n",
    "    \"min_samples_split\": list(range(2, 14))\n",
    "}]\n",
    "\n",
    "RF_Model = GridSearchCV(RandomForestRegressor(), param, cv=5, n_jobs=-1)\n",
    "RF_Model.fit(X_train, y_train_reg)\n",
    "RF_Predict = RF_Model.predict(X_test)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (RF_Model.best_score_, RF_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, RF_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, RF_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, RF_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, RF_Predict)\n",
    "#plt.title(\"Results of Random Forest Regression\")\n",
    "save_fig(\"results_of_RF_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_RF = reg_to_class(RF_Predict)\n",
    "RF_Accuracy = accuracy_score(y_test_class, y_rtc_RF)\n",
    "threshold_accuracy_score.append(RF_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(RF_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_RF, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Random Forest Regression\")\n",
    "save_fig(\"confusion_matrix_of_RF_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation par validation croisée de la valeur de *max_depth* et *min_samples_split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"max_depth\": list(range(2, 10)),\n",
    "    \"min_samples_split\": list(range(2, 10))\n",
    "}]\n",
    "\n",
    "DT_Model = GridSearchCV(DecisionTreeRegressor(), param, cv=10, n_jobs=-1)\n",
    "DT_Model.fit(X_train, y_train_reg)\n",
    "DT_Predict = DT_Model.predict(X_test)\n",
    "\n",
    "# Paramètres optimaux\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (DT_Model.best_score_, DT_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, DT_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, DT_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, DT_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, DT_Predict)\n",
    "#plt.title(\"Results of Decision Trees Regression\")\n",
    "save_fig(\"results_of_DT_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_DT = reg_to_class(DT_Predict)\n",
    "DT_Accuracy = accuracy_score(y_test_class, y_rtc_DT)\n",
    "threshold_accuracy_score.append(DT_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(DT_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_DT, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Decision Trees Regression\")\n",
    "save_fig(\"confusion_matrix_of_DT_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR\n",
    "\n",
    "#### Linear Kernel\n",
    "\n",
    "Optimisation de la pénalisation (paramètre $C$) par validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 0.5, 0.7, 0.8, 1., 1.5, 2.]\n",
    "}]\n",
    "\n",
    "Lin_SVR_Model = GridSearchCV(SVR(kernel='linear'), param, cv=5)\n",
    "Lin_SVR_Model.fit(X_train_scaled, y_train_reg)\n",
    "Lin_SVR_Predict = Lin_SVR_Model.predict(X_test_scaled)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Lin_SVR_Model.best_score_, Lin_SVR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, Lin_SVR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, Lin_SVR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, Lin_SVR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, Lin_SVR_Predict)\n",
    "#plt.title(\"Results of Linear SVR Regression\")\n",
    "save_fig(\"results_of_Lin_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_Lin_SVR = reg_to_class(Lin_SVR_Predict)\n",
    "Lin_SVR_Accuracy = accuracy_score(y_test_class, y_rtc_Lin_SVR)\n",
    "threshold_accuracy_score.append(Lin_SVR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(Lin_SVR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_Lin_SVR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Linear SVR Regression\")\n",
    "save_fig(\"confusion_matrix_of_Lin_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel\n",
    "\n",
    "Optimisation de la pénalisation (paramètre $C$) par validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 0.5, 1., 2., 5.],\n",
    "    \"degree\": [2, 3, 4]\n",
    "}]\n",
    "\n",
    "Poly_SVR_Model = GridSearchCV(SVR(kernel='poly', gamma='auto', coef0=1.), param, cv=5)\n",
    "Poly_SVR_Model.fit(X_train_scaled, y_train_reg)\n",
    "Poly_SVR_Predict = Poly_SVR_Model.predict(X_test_scaled)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Poly_SVR_Model.best_score_, Poly_SVR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, Poly_SVR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, Poly_SVR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, Poly_SVR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, Poly_SVR_Predict)\n",
    "#plt.title(\"Results of Polynomial SVR Regression\")\n",
    "save_fig(\"results_of_Poly_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_Poly_SVR = reg_to_class(Poly_SVR_Predict)\n",
    "Poly_SVR_Accuracy = accuracy_score(y_test_class, y_rtc_Poly_SVR)\n",
    "threshold_accuracy_score.append(Poly_SVR_Accuracy)\n",
    "\n",
    "print(\"Précision :\" + str(Poly_SVR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_Poly_SVR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Polynomial SVR Regression\")\n",
    "save_fig(\"confusion_matrix_of_Poly_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\n",
    "    \"C\": [0.1, 0.5, 1., 2., 5.],\n",
    "    \"gamma\": ['auto', 'scale', 1e-3, 1e-4]\n",
    "}]\n",
    "\n",
    "Rad_SVR_Model = GridSearchCV(SVR(kernel='rbf'), param, cv=5)\n",
    "Rad_SVR_Model.fit(X_train_scaled, y_train_reg)\n",
    "Rad_SVR_Predict = Rad_SVR_Model.predict(X_test_scaled)\n",
    "\n",
    "# Paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" %\n",
    "      (Rad_SVR_Model.best_score_, Rad_SVR_Model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, Rad_SVR_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, Rad_SVR_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, Rad_SVR_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, Rad_SVR_Predict)\n",
    "#plt.title(\"Results of Radial SVR Regression\")\n",
    "save_fig(\"results_of_Rad_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_Rad_SVR = reg_to_class(Rad_SVR_Predict)\n",
    "Rad_SVR_Accuracy = accuracy_score(y_test_class, y_rtc_Rad_SVR)\n",
    "threshold_accuracy_score.append(Rad_SVR_Accuracy)\n",
    "\n",
    "print(\"Précision :\" + str(Rad_SVR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_Rad_SVR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Radial SVR Regression\")\n",
    "save_fig(\"confusion_matrix_of_Rad_SVR_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseaux de neuronnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1], 1\n",
    "NN_Model = get_NN_model(n_inputs, n_outputs, 'regression')\n",
    "NN_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = NN_Model.fit(X_train_scaled_mm, y_train_reg, epochs=200, batch_size=30,\n",
    "                       validation_data=(X_test_scaled_mm, y_test_reg), verbose=0)\n",
    "NN_Predict = NN_Model.predict(X_test_scaled_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores.append(mean_squared_error(y_test_reg, NN_Predict))\n",
    "r2_scores.append(r2_score(y_test_reg, NN_Predict))\n",
    "evs_scores.append(explained_variance_score(y_test_reg, NN_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(reg_metrics, y_test_reg, y_test_class, NN_Predict)\n",
    "#plt.title(\"Results of Neural Network Regression\")\n",
    "save_fig(\"results_of_NN_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rtc_NNR = reg_to_class(NN_Predict)\n",
    "NNR_Accuracy = accuracy_score(y_test_class, y_rtc_NNR)\n",
    "threshold_accuracy_score.append(NNR_Accuracy)\n",
    "\n",
    "print(\"Précision : \" + str(NNR_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(y_test_class, y_rtc_NNR, cmap='Blues', draw_mosaic=False)\n",
    "#plt.title(\"Confusion Matrix of Neural Network Regression\")\n",
    "save_fig(\"confusion_matrix_of_NN_reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé des résultats en régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = [\n",
    "    'Linear Regression', 'Lasso', 'Ridge', 'Elastic Net', 'Random Forest',\n",
    "    'Decision Trees', 'Linear SVR', 'Polynomial SVR', 'Radial SVR',\n",
    "    'Neural Network'\n",
    "]\n",
    "\n",
    "regression_scores = pd.DataFrame({\n",
    "    'Model': reg_models,\n",
    "    'Mean Squared Error': mse_scores,\n",
    "    'R2 Score': r2_scores,\n",
    "    'Explained Variance Score': evs_scores,\n",
    "    'Thresholding Accuracy Score': threshold_accuracy_score\n",
    "})\n",
    "\n",
    "regression_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_scores.sort_values(by='Mean Squared Error', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison entre classification et régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifs = model_performance_accuracy[\"Accuracy Score\"]\n",
    "regs = regression_scores[\"Thresholding Accuracy Score\"]\n",
    "\n",
    "compare_table = pd.DataFrame({\n",
    "    'Model': class_models,\n",
    "    'Classification': classifs,\n",
    "    'Thresholding': regs\n",
    "})\n",
    "\n",
    "compare_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "428.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
